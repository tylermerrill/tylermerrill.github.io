[
  {
    "objectID": "Ansg_1_6354.html",
    "href": "Ansg_1_6354.html",
    "title": "Assignment One",
    "section": "",
    "text": "1) - I have booked a ride via an app for Greyhound bus-lines. The app used databases to store ticket holder information, purchase details, and trip times that I scheduled with them. This database information was accesed by a bus-driver in Downtown Dallas when I departed, who then arrived at the destination the database had stored, which was a state away. I have ordered clothes from the online retailer Temu. They used a databses to store my credit card information, to tell my bank to synchronously deduce the balance in my account a certain amount by their use of another proprietary DBMS, and store a record of my address to share with a third party shipping provider. Who ‘lost my clothes’ because their DBMS might have been experiencing troubles at the time. Additionally, I have also ordered many McDonald’s $5 dollar deals with their custom app, which uses a distributed database to store information such as the type of orders I have made, and uses that data to proffer deals that might appeal to me in the future.\n2) - A data base can be developed to support a health-care responsiveness app. One DB can store information on the nearest RN/MD in an area, and could be used to store geo-located requests for medical assistance. Other users could use the app to quickly ‘ping’ a call for help, and any relevant medical professionals nearby can be notified of where the ‘distress signal’ came from and be offered the opportunity to respond to it. A DBMS would be used to keep track of these requests, locations, times, responses, etc. across various relations. Essentially what is envisioned is an Uber for lite medical care that might be faster than other emergency services. A dashboard could be developed that would be supported by the relations to map such interactions in real time.\nPerhaps a DB could be implemented to geo-locate frequently reported deposit sites of roadway trash. This would compose multiple realtions; one concerning the locations reported, another describing the type of detritus/debris. This DB could be implemented within pre-existing civil infrastructure or provisioned by a commercial network. Information could be input via online forms or through and layered response query on an app. Locations of debris could be sent directly to driver’s via text or to higher-level management tiers for review.\nAnother DB could be utilized to simply organize information on how much sleep someone gets each day, and compare it to measured ozone levels. This might be of assistance in conducting bio-medical research into the effect of air quality on life quaility. This DB could be locally stored within a research institution conducting such an inquiry, using data on ozone layers captured daily from relevant government sites. Each of these facets of the program of study (patient info, ozone info, etc.) would of course be stored in various relations. It would probably be most accessible via a front-end application running on an end-user desktop terminal in a lab/clinic setting.\n3) Because most complex statistical agglomerations and parsings (such as determining the interquartile range of a feature-set) can best be preformed utilizing specialized statistical software such as R or Python that interface with DBMSs’ dynamically. Query languages are really only intended to ease the work of having to figure out how to sieve through large volumes of normalized data, but are not structured to do comparatively complex operations on those data themselves.\n6) A DB for a social network such as Twitter might utilize relations (tables) covering post stats, user information, and advertiser target demographics. So certain posts that reach certain target users can be used to trace an ad-stream strategy to bring any number of Twitter sponsors revenue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This domain serves as a repository of my work, including samples from my academic and professional portfolios. Please feel free to browse and explore.\n\nNotice: This is a grad student’s personal project silo. As such, it comes with no guarantees of any descriptor."
  },
  {
    "objectID": "index.html#repo-brief",
    "href": "index.html#repo-brief",
    "title": "Welcome!",
    "section": "",
    "text": "This domain serves as a repository of my work, including samples from my academic and professional portfolios. Please feel free to browse and explore.\n\nNotice: This is a grad student’s personal project silo. As such, it comes with no guarantees of any descriptor."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Selections",
    "section": "",
    "text": "The following is a sample of my work:\n \n\nPolicy Brief: [PDF] \nPCA Pass-through: [Article Link]"
  },
  {
    "objectID": "old_4.html",
    "href": "old_4.html",
    "title": "Assignment_04",
    "section": "",
    "text": "In this article, I will do some PCA (Principal Component) and clustering analysis on U.S. crime data.\nWith PCA we try to find permutations of a data-set that identifies what relationships are responsible for causing the most uncorrelated variance. Specifically, this transformation is done for ‘dimensions’ of the data by rotating the original data (making it orthogonal and hence uncorrelated to itself) along a new axis in a way that minimizes the average (squared) distance between observations and the new direction so taken. Often, you can work with the first two dimensions that will permit this type of transformation to make comparisons about your data-set.\nSo, I first load in my data-set:\n\nlibrary(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates = row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n\nI get the means\n\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nI also grab the variations\n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n\nI then apply PCA with scaling\n\npr.out = prcomp(USArrests, scale = TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nThen I center the data\n\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\n\nI make a ‘variable loadings matrix’\n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\n\nthen rotate it\n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\ncheck dimension size\n\ndim(pr.out$x)\n\n[1] 50  4\n\n\nFinally, outputting:\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\n\n\n\n\n\ncompute the standard deviation\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\ncalculate the variance by squaring and specifiy the proportion of variance explained\n\npr.var=pr.out$sdev^2\npve=pr.var/sum(pr.var)\n\nplotting:\n\nplot(pve, xlab=\"Principla Component\", ylab = \"Proportion of Variance Explained\", ylim=c(0,1),type = 'b')\n\n\n\n\n\n\n\n\nto show the cumulative effect of explanatory power in regard to variance, I use:\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n\n\n\n\n\nFinally, plotting out:\n\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var = \"firebrick1\")\n\n\n\n\n\n\n\n\nNext, I move on to K-Means clustering.\n\nThe goal of K-Means is to find the amount of clusters of observations within the data the minimizes the sum of squares within those clusters. Of course, we can also specify the number of clusters as well. First, I load in my new data-set, perform other set-up steps and chart the difference between hard-drive size and ram capacity in each PC:\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nlibrary(animation)\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\")\n\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n  \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\nNext, I produce the output of the k-means process via an animation with (producing the output as a sequence of frames):\n\nset.seed(2345)\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, I try the k-means approach with some flower data via the ‘Iris’ data-set:\n\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\n\n\n\n\nHere, we can see the result of grouping the real data by flower species. Now, setting up our algorithm:\n\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nChecking the class:\n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\n\nand comparing the successfullness of the k-means approach with a confusion matrix:\n\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1     50          0         0\n  2      0          2        46\n  3      0         48         4\n\n\nFinally, outputting the graphical results of the k-means approach compared with the real data:\n\nlibrary(grid)\nlibrary(gridExtra)\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\n\n\n\n\nFinally, let’s try some wine…or at least an analysis of some wine data.\n\nSetting up:\n\nlibrary(readr)\nwine &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\nwine_subset &lt;- scale(wine[ , c(2:4)])\nwine_cluster &lt;- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\nK-means clustering with 3 clusters of sizes 48, 60, 70\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.1470536  1.3907328  0.2534220\n2  0.8914655 -0.4522073  0.5406223\n3 -0.8649501 -0.5660390 -0.6371656\n\nClustering vector:\n  [1] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n [38] 2 3 1 2 1 2 1 3 1 1 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2\n [75] 3 3 3 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 1 3 3 3 3 3 1 3 3 2 1 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 1 1 1 2 1 1 1 1 1 1\n[149] 1 1 1 1 2 1 3 1 1 1 2 2 1 1 1 1 2 1 1 1 2 1 3 3 2 1 1 1 2 1\n\nWithin cluster sum of squares by cluster:\n[1]  73.71460  67.98619 111.63512\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nPlotting within-cluster sum of squares\n\nwssplot &lt;- function(data, nc=15, seed=1234){\n  wss &lt;- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] &lt;- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\nwssplot(wine_subset, nc = 9)\n\n\n\n\n\n\n\n\nNow, to plot by dimensions:\n\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\n\n\n\n\n\n\n\n\nInspect:\n\ntable(wine_cluster$cluster)\n\n\n 1  2  3 \n48 60 70 \n\n\nGraph impact of clusters #s:\n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nFancifully:\n\nwine.km &lt;- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n\n\n\n\n\n\n\nPrint output of doing k-means via ‘eclust’:\n\nwine.km\n\nK-means clustering with 3 clusters of sizes 60, 70, 48\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.8914655 -0.4522073  0.5406223\n2 -0.8649501 -0.5660390 -0.6371656\n3  0.1470536  1.3907328  0.2534220\n\nClustering vector:\n  [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n [38] 1 2 3 1 3 1 3 2 3 3 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 1 1\n [75] 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 3 2 2 2 2 2 3 2 2 1 3 3 3 2 2 2 2 3 2 3 2 3 2 2 3 3 3 3 3 1 3 3 3 3 3 3\n[149] 3 3 3 3 1 3 2 3 3 3 1 1 3 3 3 3 1 3 3 3 1 3 2 2 1 3 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1]  67.98619 111.63512  73.71460\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"clust_plot\"  \n[11] \"silinfo\"      \"nbclust\"      \"data\"         \"gap_stat\"    \n\n\nFind the optimal # of clusters using gap stats:\n\nwine.km$nbclust\n\n[1] 3\n\n\nGraph the optimality range:\n\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\n\nMake a silhouette plot\n\nfviz_silhouette(wine.km)\n\n  cluster size ave.sil.width\n1       1   60          0.44\n2       2   70          0.33\n3       3   48          0.30\n\n\n\n\n\n\n\n\n\nFinally:\n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\nAs a little bonus, we will do some hierarchical clustering with the crime data we started with.\n\nThe purpose of such clustering is to find the attribute of observations that have the smallest ‘eludican distance’ from one another, measured along some dimension of the data. Then build up from that with the next closest pairing, etc (although there are other possible building methods one could use). We first run the algorithm after scaling all variables and computing their ‘euclidean distance’ from each other along various dimensions, and then generate a dendrogram of the output:\n\nlibrary(cluster)\narrest.hc &lt;- USArrests %&gt;%\n  scale() %&gt;%                    \n  dist(method = \"euclidean\") %&gt;% \n  hclust(method = \"ward.D2\") \n\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\nWe can see certain states are found the be more ‘natural’ co-habitors via this process than others."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a grad student at the University of Texas at Dallas. I am honored to be apart of the dedicated student body of such a pivotal institution. With some of Texas’s foremost instructors, I am continuously learning the skills needed to tackle the development and sustainability challenges facing dynamic organizations in the 21st century. I hope to use these assets to assist enterprises and local governments with the ever changing needs of growth and managed progress.\n\n————————————————————————\n” It seems as for a Model laid,\nBut squint your eyes,\nAnd that the World by it was made.\nand hay can look like needles.”\n- Marvell, Anon"
  }
]